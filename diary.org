* 201301301752-LTI-PROJETO-TM
Objetivo: gerar $weights$ de 3v2 para reuso.

diff keepaway.sh.original keepaway.sh
23,24c23,24
< keeper_learn=0                   # should learning be turned on for keepers?
< #keeper_policy="learned"         # policy followed by keepers
---
> keeper_learn=1                   # should learning be turned on for keepers?
> keeper_policy="learned"          # policy followed by keepers
27c27
< keeper_policy="rand"
---
> #keeper_policy="rand"
44c44
< save_weights=0                    # should I save learned weights
---
> save_weights=1                    # should I save learned weights
63c63
< synch_mode=0                     # should I speed up with synchronous mode?
---
> synch_mode=1                     # should I speed up with synchronous mode?
66c66
< save_rcg_log=0                   # should I save game log to .rcg file?
---
> save_rcg_log=1                   # should I save game log to .rcg file?
* 201301301958-LTI-PROJETO-TM
Objetivo: testar reuso de $weights$

diff keepaway.sh.original keepaway.sh
21,22c21,22
< keeper_load=0                    # should I load previously learned weights?
< keeper_load_dir=                 # sub-directory of weight_dir where weights are stored
---
> keeper_load=1                    # should I load previously learned weights?
> keeper_load_dir=201301301752-LTI-PROJETO-TM                 # sub-directory of weight_dir where weights are stored
24c24
< #keeper_policy="learned"         # policy followed by keepers
---
> keeper_policy="learned"          # policy followed by keepers
27c27
< keeper_policy="rand"
---
> #keeper_policy="rand"
44c44
< save_weights=0                    # should I save learned weights
---
> save_weights=1                    # should I save learned weights
63c63
< synch_mode=0                     # should I speed up with synchronous mode?
---
> synch_mode=1                     # should I speed up with synchronous mode?
66c66
< save_rcg_log=0                   # should I save game log to .rcg file?
---
> save_rcg_log=1                   # should I save game log to .rcg file?
* 201301302106-LTI-PROJETO-TM
Objetivo: obter weights de 3v2 muito bons.

diff keepaway.sh.original keepaway.sh
22,24c22,24
< keeper_load_dir=                 # sub-directory of weight_dir where weights are stored
< keeper_learn=0                   # should learning be turned on for keepers?
< #keeper_policy="learned"         # policy followed by keepers
---
> keeper_load_dir=201301301752-LTI-PROJETO-TM                 # sub-directory of weight_dir where weights are stored
> keeper_learn=1                   # should learning be turned on for keepers?
> keeper_policy="learned"          # policy followed by keepers
27c27
< keeper_policy="rand"
---
> #keeper_policy="rand"
44c44
< save_weights=0                    # should I save learned weights
---
> save_weights=1                    # should I save learned weights
63c63
< synch_mode=0                     # should I speed up with synchronous mode?
---
> synch_mode=1                     # should I speed up with synchronous mode?
66c66
< save_rcg_log=0                   # should I save game log to .rcg file?
---
> save_rcg_log=1                   # should I save game log to .rcg file?
* 201302131649-LTI-PROJETO-TM
3v2.

Geração de pesos.

Testes da nova estrutura de treinamento.

Já com nova estrutura de estados, que foi feita para a transferência
de conhecimento.

Nessa representação, as novas features presentes em 4v3 são alocadas
ao final do vetor.

Não há clonagem de pesos para a ação /passK4/ nesse aprendizado.
* 201302141237-LTI-PROJETO-TM
Mais um 3v2 pra comparar o aprendizado antes e depois da modificação.
* 201302141632-LTI-PROJETO-TM
Mais um 3v2 pra comparar o aprendizado antes e depois da modificação.
* 201302142138-LTI-PROJETO-TM
4v3 na nova representação
* 201302150114-LTI-PROJETO-TM
4v3 reusando 201302141237-LTI-PROJETO-TM.
Atentando que somente 3 dos 4 jogadores abriram weights.
Inicialmente o desempenho é ruim (transf. negativa).

Possivelmente tentar posteriormente copiar k3-weights para k4-weights
(se eu não me engano, foi assim que o Fernandez'10 fez).

O HD ficou cheio.

Deletei os .cfg da pasta logs após o início do experimento.
* 201302151146-LTI-PROJETO-TM
4v3 reusando 201302141237-LTI-PROJETO-TM.
Copiado k3-weghts para k4-weights.
Isso traz como consequencia: o quarto jogador joga da mesma forma que
o terceiro.
Entretanto, não corrige a negligência à ação passK4.
* 201302161522-LTI-PROJETO-TM
Aprendizado 3v2, só que aprendendo os pesos para a ação pass_k4.

O reuso realizado em 201302151146-LTI-PROJETO-TM não apresentou
resultados positivos.

Acredito que o problema do aprendizado está na negligência à ação
pass_k4, que inicialmente está desprovida de pesos.

Para corrigir isso, modifiquei o fonte de forma a clonar os pesos
aprendidos para a pass_k3 diretamente em pass_k4.

Dessa forma, no reuso, os jogadores valorizariam a ação pass_k4 tanto
quanto a ação pass_k3.

Com isso, espero corrigir o problema no aprendizado.
* 201302162051-LTI-PROJETO-TM
4v3 reusando 201302161522-LTI-PROJETO-TM.

Como foram aprendidos os pesos para pass_k4, espero um desempenho de
aprendizado melhor do que o "puro".

RESULTADO: Positivo, pois o reuso implicou um aprendizado mais
eficiente do que o "puro".
* 201302161101-LTI-PROJETO-TM
4v3 reusando 201302161522-LTI-PROJETO-TM.

Repetição de 201302162051-LTI-PROJETO-TM.

RESULTADO: compatível com 201302162051-LTI-PROJETO-TM.
* 201302261429-LTI-PROJETO-TM
Teste do módulo 'learning from scratch' para o PRQL.  Isolei o reuso
para o caso em que há mais de uma política na biblioteca. Caso
contrário, o aprendizado ocorre como na versão original do programa.

Os resultados iniciais não sugerem nada que indique incorreção.
* 201302261524-LTI-PROJETO-TM
Teste PRQL. 4v3 reusando 3v2

PROBLEMA: Os valores utilizados no cálculo de Boltzmann ficavam muito grandes
muito rápido.
* 201302261621-LTI-PROJETO-TM
Teste PRQL. 4v3 reusando 3v2

Modificado tau_increment de 0.05 para 0.0005.
* 201302261709-LTI-PROJETO-TM
Teste PRQL. 4v3 reusando 3v2

Modificado tau_increment=0.00005'
* 201302261807-LTI-PROJETO-TM
Aprendizado from scratch 4v3.
Utilizando epsilon e epsilon_increment de acordo com Fernandez'10.
Será utilizado para comparação com reuso.
* 201302270325-LTI-PROJETO-TM
4v3 reusando 3v2 após correção de bug que fazia com que a política que
estava sendo aprendida não fosse explotada no reuso.

tau_increment 0.025

Resultado: aprendeu muito lentamente.
* 201302271649-LTI-PROJETO-TM
Repetição do 201302270325-LTI-PROJETO-TM.
Modificado tau_increment para 0.05 (o mesmo do artigo).
* 201302272034-LTI-PROJETO-TM
Idem.
Modificado tau_increment para 0.005.
* 201302272101-LTI-PROJETO-TM
Idem.
Modificado tau_increment para 0.00008.
* 201302281201-LTI-PROJETO-TM
Idem.
Modificado tau_increment para 0.0000008.
* 201302281304-LTI-PROJETO-TM
Idem.
Modificado tau_increment para 0.000008.
* 201302281357-LTI-PROJETO-TM
Reverti a selectAction para o jeito antigo para ver se o resultado é o
mesmo de antes.

Parâmetros exatamente iguais aos do Fernandez'10.
* 201303021838-LTI-PROJETO-TM
4v3 PRQL 3v2 só que os pesos não sofreram o clone do Taylor.
tau_increment 0.005

BUG: a parte que explota a política que está sendo aprendida estava
comentada (a parte que corrigia isso...).
* 201303030229-LTI-PROJETO-TM
Percebi que havia um bug: a parte que explota a política que está
sendo aprendida estava comentada (a parte que corrigia isso...).

tau_increment 0.05

PROBLEMA: valores de e^tau*W ficam muito altos, chegam a infinito, e o
sistema acaba escolhendo a política errada para explotar.
* 201303042104-LTI-PROJETO-TM
Depois de várias correções no código.

Rodando com os parâmetros do Fernández'10.

Inicialmente obteve desempenho similar ao aprendizado cru.
Posteriormente (~12h de treino) obteve desempenho inferior ao
aprendizado cru.
* 201303042241-LTI-PROJETO-TM
Modifiquei o tau_increment para 0.009 para ver se melhorava.

Obteve desempenho similar ao aprendizado cru no início, entretanto
PIOR do que utilizando tau_increment = 0.05.

Outra coisa que observei foi que, após um período considerável de
tempo, a probabilidade de reuso da política 0, ou seja, a que está
sendo aprendida, fica muito próxima ou igual a zero.

IDEIA: Acho que uma boa seria plotar as quantidades envolvidas,
pricipalmente os valores relacionados à escolha da política a ser
explotada.

Isso auxiliaria na verificação do comportamento do reuso. 

IDEIA: fazer ele só reusar. Teoricamente, deve ter um desempenho
inicial melhor do que o cru, porque já aprendeu algo. (Mas há outros
componentes... Eles não contariam?)

IDEIA: Reusar a política 3v2 em um jogo 3v2.

IDEIA: O erro não está nos parâmetros, mas na sua evolução. E isso
pode ser decorrente do meu código.

IDEIA: O gráfico do Fernández'10 foi plotado de forma diferente.

IDEIA: O meu cru não tá igual ao dele. Ele chega em ~12s após 50h de
treino e eu chego somente em ~10 e ~11. Rodar o aprendizado cru na
versão original do tjpalmer e com todos aprendendo from scratch na
minha versão mais atual.
